# ============================================================================
# MODIFICATIONS LOG - GitHub Copilot
# ============================================================================
# Date: 2025-11-30
# Changes:
#   1. Modified dataset indexing to handle both 'output' subfolder and direct
#      folder structure (for Mini_Dataset compatibility)
#   2. Added try/except for energy parsing to skip invalid folder names
#   3. Added check for batch_path being a directory
#   4. Modified setup() to handle small datasets (<3 samples)
#   5. Added print statements for dataset size debugging
#   6. Changed test_set subset to use min(200, actual_size)
# Why: Mini_Dataset has different folder structure and very few samples
# ----------------------------------------------------------------------------
# Date: 2025-12-02
# Changes:
#   7. Added add_poisson_noise() function for simulating Low-Photon MC
#   8. Added num_photons and add_noise parameters to ConditionalDoseDataset
#   9. Modified __getitem__ to create LP dose from HP dose using Poisson noise
#   10. Added lp_dose to condition dict for Diff-MC pipeline
# Why: Adapt to Diff-MC paper (Professor Hesser Session 3):
#      "Take that as mean value and then sample... and then you generate 
#       Poisson images, because this is easy to go."
#      Data analysis showed: input_cubes = CT (-1000 to 1207)
#                           output_cubes = Dose (0.000001 to 0.017557)
# ----------------------------------------------------------------------------
# Date: 2025-12-03
# Changes:
#   11. Updated dataset to load pre-generated LP dose from lp_cubes/ folder
#   12. Added use_pregenerated_lp option to switch between:
#       - True: Load LP from lp_cubes/ (pre-generated by generate_lp_dose.py)
#       - False: Generate LP on-the-fly from HP using Poisson noise
#   13. Restructured __getitem__ to clearly separate CT, LP, HP
#   14. Added detailed comments explaining the Diff-MC pipeline
# Why: User requested file-based LP dose storage for reproducibility
#      and to match Diff-MC paper methodology.
# 
# Data Flow (Diff-MC style):
#   input_cubes/   → CT volume    (anatomy, HU: -1000 to 1200)
#   lp_cubes/      → LP dose      (noisy input, pre-generated)
#   output_cubes/  → HP dose      (clean target, ground truth)
#
# Model learns: f(CT, LP, energy) → HP  (conditional denoising)
# ============================================================================

import os
import numpy as np
import torch
from torch.utils.data import Dataset
from tqdm import tqdm
import glob
import pytorch_lightning as pl
from torch.utils.data import random_split, DataLoader
import torch.nn.functional as F


# ============================================================================
# Date: 2025-12-26
# Function: add_poisson_noise (CORRECTED per Prof. Hesser Dec 15 meeting)
# Why: Professor Hesser clarified the correct Poisson noise formulation:
#      
#      "Let's see we have a dose D to a voxel. We have to find the number 
#       of particles which are there. So N = D / δ, where δ is the average 
#       dose per particle. Then you do a Poisson simulation: N' ~ Poisson(N).
#       And the dose which is simulated: D' = N' × δ"
#
# Correct Formula (from Hesser's whiteboard):
#      1. N = D / δ          (calculate number of particles)
#      2. N' ~ Poisson(N)    (sample from Poisson distribution)
#      3. D' = N' × δ        (convert back to dose)
#
# Implementation Notes:
#   - If delta is provided: use it directly (physical interpretation)
#   - If delta is None: auto-calculate from target_uncertainty
#   - Auto-calculation: δ = max_dose / target_particles
#     where target_particles = (1 / target_uncertainty)²
#     Example: For 10% uncertainty → 100 particles → δ = max_dose / 100
#
# Physical Equivalence:
#   Hesser's delta approach is mathematically equivalent to scaling factor N:
#       δ = 1 / N
#   Example: N=37000 ↔ δ=2.7e-5 (for max_dose=0.0027, target=100 particles)
# ============================================================================
def add_poisson_noise(clean_dose: np.ndarray, 
                     delta: float = None, 
                     target_uncertainty: float = 0.10) -> np.ndarray:
    """
    Simulate Low-Photon Monte Carlo using Prof. Hesser's Poisson formulation.
    
    Args:
        clean_dose: High-Photon MC dose in physical units (D, in Gy)
        delta: Average dose per particle interaction (δ, in Gy)
               If None, auto-calculates from target_uncertainty and max_dose
        target_uncertainty: Target relative uncertainty (default: 0.10 = 10%)
                           Only used if delta=None
    
    Returns:
        noisy_dose: Low-Photon MC dose (D', in Gy)
        
    Physical Interpretation:
        - For 10% uncertainty: need ~100 particles (since σ_rel = 1/√N)
        - At max_dose = 0.0027 Gy: δ = 0.0027 / 100 = 2.7e-5 Gy
        - This gives: High dose → ~100 particles (10% noise)
                     Mid dose  → ~40 particles  (16% noise)
                     Low dose  → ~10 particles  (32% noise)
    
    Example:
        # Manual delta (if known from simulation)
        lp_dose = add_poisson_noise(hp_dose, delta=1e-5)
        
        # Auto-calculate delta for 10% uncertainty
        lp_dose = add_poisson_noise(hp_dose, target_uncertainty=0.10)
    """
    # Ensure non-negative (dose cannot be negative)
    clean_dose = np.maximum(clean_dose, 0)
    
    # Auto-calculate delta if not provided
    if delta is None:
        max_dose = clean_dose.max()
        if max_dose > 0:
            target_particles = (1.0 / target_uncertainty) ** 2
            delta = max_dose / target_particles
        else:
            # Fallback for zero dose
            delta = 1e-5
    
    # Step 1: Calculate number of particles per voxel
    # N = D / δ
    N_particles = clean_dose / delta
    
    # Step 2: Sample from Poisson distribution
    # N' ~ Poisson(N)
    N_particles = np.maximum(N_particles, 0)
    N_noisy = np.random.poisson(N_particles).astype(np.float32)
    
    # Step 3: Convert back to dose
    # D' = N' × δ
    noisy_dose = N_noisy * delta
    
    return noisy_dose


class ConditionalDoseDataset(Dataset):
    # ========================================================================
    # Date: 2025-12-03
    # Changes: Added use_pregenerated_lp parameter
    # Date: 2025-12-26
    # Changes: Replaced num_photons with delta and target_uncertainty
    # Why: Support both pre-generated LP (reproducible) and on-the-fly LP
    #      Using Prof. Hesser's delta formulation instead of num_photons
    # ========================================================================
    def __init__(self, root_dir, normalize=True, target_dim=None,
                 delta=None, target_uncertainty=0.10, add_noise=True,
                 use_pregenerated_lp=True, lp_folder: str = "lp_cubes",
                 dose_scale: float = 0.02):
        """
        Diff-MC style dataset for Monte Carlo dose denoising.
        
        Data folders:
            input_cubes/   → CT volume (anatomy, HU values)
            output_cubes/  → HP dose (High-Photon, clean, target)
            lp_cubes/      → LP dose (Low-Photon, noisy, input) [if use_pregenerated_lp=True]
        
        Args:
            root_dir: Path to the dataset root.
            normalize: Whether to normalize each volume individually.
            target_dim: Target spatial dimension (e.g., 128 → 128³)
            delta: Dose per particle (δ, in Gy). If None, auto-calculated.
            target_uncertainty: Target relative uncertainty (e.g., 0.10 = 10%)
                               Used for auto-calculating delta if not provided.
            add_noise: Whether to add noise (False for clean LP = HP)
            use_pregenerated_lp: If True, load LP from lp_cubes/ folder.
                                 If False, generate LP on-the-fly from HP.
            lp_folder: Folder name under each batch directory that contains LP dose
                       volumes (default: "lp_cubes"). Useful for variants like
                       "lp_cubes_100".
        """
        super().__init__()
        self.data = []
        self.normalize = normalize
        self.target_dim = target_dim
        self.delta = delta
        self.target_uncertainty = target_uncertainty
        self.add_noise = add_noise
        self.use_pregenerated_lp = use_pregenerated_lp  # Date: 2025-12-03
        self.lp_folder = lp_folder
        self.dose_scale = float(dose_scale)

        # ====================================================================
        # Date: 2026-01-06: Support FLAT structure (patient_id/input_cubes)
        # The dataset can have two structures:
        #   1. NESTED: energy/batch_id/input_cubes/*.npy (original)
        #   2. FLAT:   patient_id/input_cubes/*.npy (current data)
        # We detect this by checking if folders contain input_cubes directly
        # ====================================================================
        patient_folders = sorted(os.listdir(root_dir))
        for patient_folder in tqdm(patient_folders, desc="Indexing dataset"):
            patient_path = os.path.join(root_dir, patient_folder)
            
            if not os.path.isdir(patient_path):
                continue
            
            # Check if this folder directly contains input_cubes (FLAT structure)
            input_path = os.path.join(patient_path, "input_cubes")
            output_path = os.path.join(patient_path, "output_cubes")
            lp_path = os.path.join(patient_path, self.lp_folder)
            
            if os.path.isdir(input_path) and os.path.isdir(output_path):
                # FLAT structure: patient_id/input_cubes/*.npy
                energy = 0.0  # Default energy for this dataset
                
                input_files = sorted(glob.glob(os.path.join(input_path, "*.npy")))
                for file in input_files:
                    filename = os.path.basename(file)
                    ct_file = os.path.join(input_path, filename)
                    hp_file = os.path.join(output_path, filename)
                    lp_file = os.path.join(lp_path, filename)
                    
                    # Skip file existence check for speed - assume LP exists
                    self.data.append((ct_file, hp_file, lp_file, energy))
            else:
                # NESTED structure: energy/batch_id/input_cubes/*.npy
                try:
                    energy = float(patient_folder.replace("_", "."))
                except ValueError:
                    continue
                
                for batch_id in os.listdir(patient_path):
                    batch_path = os.path.join(patient_path, batch_id)
                    if not os.path.isdir(batch_path):
                        continue
                    
                    input_path = os.path.join(batch_path, "input_cubes")
                    output_path = os.path.join(batch_path, "output_cubes")
                    lp_path = os.path.join(batch_path, self.lp_folder)

                    input_files = sorted(glob.glob(os.path.join(input_path, "*.npy")))
                    for file in input_files:
                        filename = os.path.basename(file)
                        ct_file = os.path.join(input_path, filename)
                        hp_file = os.path.join(output_path, filename)
                        lp_file = os.path.join(lp_path, filename)
                        
                        # Skip file existence check for speed
                        self.data.append((ct_file, hp_file, lp_file, energy))

    def __len__(self):
        return len(self.data)

    def reshape_tensor(self, tensor):
        """Resizes a 4D tensor (1, D, H, W) to (1, target_dim, target_dim, target_dim)."""
        if self.target_dim is None:
            return tensor
        return F.interpolate(tensor.unsqueeze(0), size=(self.target_dim,) * 3, mode='trilinear', align_corners=False).squeeze(0)

    # ========================================================================
    # Date: 2025-12-03
    # Changes: Complete rewrite of __getitem__ for Diff-MC pipeline
    # 
    # Data Loading:
    #   - CT volume:  from input_cubes/  (anatomy, HU: -1000 to 1200)
    #   - HP dose:    from output_cubes/ (clean, ground truth)
    #   - LP dose:    from lp_cubes/ or generated on-the-fly
    #
    # Model Input/Output:
    #   - Input (condition): CT + LP dose + beam energy
    #   - Output (target):   HP dose
    #
    # Channel Organization:
    #   - Each volume is [1, D, H, W] (single channel, 3D)
    #   - Condition dict contains separate tensors for CT and LP
    #   - Model can concatenate [CT, LP] as 2-channel input if needed
    # ========================================================================
    def __getitem__(self, idx):
        ct_file, hp_file, lp_file, energy = self.data[idx]
        
        # ====================================================================
        # Load CT volume (anatomy)
        # Source: input_cubes/
        # Range: -1000 (air) to ~1200 (bone) HU
        # Purpose: Anatomical context for dose prediction
        # ====================================================================
        ct_vol = np.load(ct_file).astype(np.float32)
        
        # ====================================================================
        # Load HP dose (High-Photon = clean = target)
        # Source: output_cubes/
        # Range: ~1e-6 to ~1e-2 (dose values)
        # Purpose: Ground truth for training (what model should predict)
        # ====================================================================
        hp_dose = np.load(hp_file).astype(np.float32)
        
        # ====================================================================
        # Load or Generate LP dose (Low-Photon = noisy = input)
        # Source: lp_cubes/ (pre-generated) or on-the-fly from HP
        # Range: Similar to HP but with Poisson noise
        # Purpose: Noisy input for denoising task
        # ====================================================================
        if self.use_pregenerated_lp and lp_file is not None:
            # Load pre-generated LP dose (reproducible)
            lp_dose = np.load(lp_file).astype(np.float32)
        elif self.add_noise:
            # Generate LP dose on-the-fly (random each epoch)
            lp_dose = add_poisson_noise(hp_dose, 
                                       delta=self.delta, 
                                       target_uncertainty=self.target_uncertainty)
        else:
            # No noise (for testing/ablation)
            lp_dose = hp_dose.copy()
        
            # ====================================================================
        # Normalize each volume (z-score normalization)
        # IMPORTANT: Store normalization stats for un-normalization in testing
        # 
        # CRITICAL FIX: Only normalize CT, keep dose in physical units!
        # Why: Per-sample dose normalization loses absolute scale information.
        # The model cannot learn the relationship between dose values across samples.
        # ====================================================================
        hp_mean = hp_dose.mean()
        hp_std = hp_dose.std() + 1e-8
        
        if self.normalize:
            # Normalize CT (HU values)
            ct_vol = (ct_vol - ct_vol.mean()) / (ct_vol.std() + 1e-5)
            
            # Scale dose to [0, 1] range using a fixed global max
            # Typical dose range: 0 to ~0.01 Gy
            # Use 0.02 as safe upper bound
            lp_dose = np.clip(lp_dose / self.dose_scale, 0, 1)
            hp_dose = np.clip(hp_dose / self.dose_scale, 0, 1)

        # ====================================================================
        # Convert to PyTorch tensors
        # Shape: [1, D, H, W] (1 channel, 3D volume)
        # ====================================================================
        ct_tensor = torch.from_numpy(ct_vol).unsqueeze(0)      # [1, D, H, W]
        lp_tensor = torch.from_numpy(lp_dose).unsqueeze(0)     # [1, D, H, W]
        hp_tensor = torch.from_numpy(hp_dose).unsqueeze(0)     # [1, D, H, W]

        # ====================================================================
        # Reshape to target dimension if specified
        # ====================================================================
        ct_tensor = self.reshape_tensor(ct_tensor)
        lp_tensor = self.reshape_tensor(lp_tensor)
        hp_tensor = self.reshape_tensor(hp_tensor)

        # ====================================================================
        # Construct condition dictionary
        # 
        # The model receives this dict as conditioning information:
        #   - "ct": CT volume for anatomical context
        #   - "lp_dose": Low-Photon dose (noisy input to be denoised)
        #   - "energy": Beam energy in keV
        #   - "hp_mean", "hp_std": For un-normalization in testing
        #
        # Model architectures can use this in different ways:
        #   1. Concatenate [CT, LP] as 2-channel input
        #   2. Use CT as cross-attention context
        #   3. Use energy as class embedding
        # ====================================================================
        condition = {
            "ct": ct_tensor,                                    # [1, D, H, W] - Anatomy
            "lp_dose": lp_tensor,                               # [1, D, H, W] - Noisy input
            "energy": torch.tensor([energy], dtype=torch.float32),  # [1] - Beam energy
            "hp_mean": torch.tensor([hp_mean], dtype=torch.float32),  # For un-normalization
            "hp_std": torch.tensor([hp_std], dtype=torch.float32),    # For un-normalization
            "dose_scale": torch.tensor([self.dose_scale], dtype=torch.float32),
        }

        # ====================================================================
        # Return: (target, condition)
        #   - target = HP dose (what model should predict)
        #   - condition = dict with CT, LP, energy (model input)
        # ====================================================================
        return hp_tensor, condition


class DoseDataModule(pl.LightningDataModule):
    def __init__(self, root_dir, batch_size=4, num_workers=4, split=(0.8, 0.1, 0.1), target_dim=None):
        super().__init__()
        assert sum(split) == 1.0, "Splits must sum to 1."
        self.root_dir = root_dir
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.split = split
        self.target_dim = target_dim

    def setup(self, stage=None):
        full_dataset = ConditionalDoseDataset(self.root_dir, target_dim=self.target_dim)

        total = len(full_dataset)
        print(f"Total dataset size: {total} samples")
        
        # Handle small datasets by ensuring at least 1 sample in each split
        if total < 3:
            # For very small datasets, use same data for all splits
            train_len = total
            val_len = 0
            test_len = 0
            self.train_set = full_dataset
            self.val_set = full_dataset
            self.test_set = full_dataset
        else:
            train_len = max(1, int(self.split[0] * total))
            val_len = max(1, int(self.split[1] * total))
            test_len = total - train_len - val_len
            if test_len < 1:
                test_len = 1
                train_len = total - val_len - test_len
            
            self.train_set, self.val_set, self.test_set = random_split(
                full_dataset, [train_len, val_len, test_len]
            )

        print(f"Train: {len(self.train_set)}, Val: {len(self.val_set)}, Test: {len(self.test_set)}")

        # Limit test dataset to min(200, actual size)
        test_size = min(200, len(self.test_set))
        if test_size > 0:
            self.test_set = torch.utils.data.Subset(self.test_set, list(range(test_size)))

    def train_dataloader(self):
        return DataLoader(self.train_set, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)

    def val_dataloader(self):
        return DataLoader(self.val_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)

    def test_dataloader(self):
        return DataLoader(self.test_set, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)


if __name__ == '__main__':
    # each energy has 10 batches
    # each batch has 10 patients (100 total patients)
    # 200 patients cubes per patient
    # = 160 000 sample cubes (100x100x100) in the entire dataset
    import random

    # Point this to your dataset root
    root_dir = "/hdd/Josch_Data/simulations"

    # Instantiate the dataset
    dataset = ConditionalDoseDataset(root_dir=root_dir, normalize=True)

    print(f"✅ Loaded dataset with {len(dataset)} samples")

    # Pick a random sample
    index = random.randint(0, len(dataset) - 1)
    input_tensor, condition, target_tensor = dataset[index]

    print("\n--- Sample Inspection ---")
    print(f"Index: {index}")
    print(f"Input CT shape:        {input_tensor.shape} (should be [1, D, H, W])")
    print(f"Target Dose shape:     {target_tensor.shape} (should be [1, D, H, W])")
    print(f"Condition 'ct' shape:  {condition['ct'].shape} (copy of input)")
    print(f"Condition 'energy':    {condition['energy'].item()} keV")

    assert input_tensor.shape == target_tensor.shape, "❌ Input and target shapes do not match!"
    assert condition["ct"].shape == input_tensor.shape, "❌ Condition CT shape mismatch!"
    assert isinstance(condition["energy"], torch.Tensor), "❌ Energy is not a tensor!"

    print("✅ All checks passed.")